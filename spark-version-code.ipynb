{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ml-bank').getOrCreate()\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '40g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '1'), ('spark.cores.max', '1'), ('spark.driver.memory','50g')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherDF= spark.read.csv('/home/negah.moharrami@analytique.desjardins.com/project/CEBD1261/data/processed/New York_Weather_cyclical_taxi.csv', header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rides</th>\n",
       "      <td>8760</td>\n",
       "      <td>36.70616438356164</td>\n",
       "      <td>7.606905749665114</td>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>8760</td>\n",
       "      <td>11.5</td>\n",
       "      <td>6.922581688234331</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>8760</td>\n",
       "      <td>6.526027397260274</td>\n",
       "      <td>3.448048134069238</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>8760</td>\n",
       "      <td>15.72054794520548</td>\n",
       "      <td>8.796749115212322</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>8760</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0                  1                  2     3     4\n",
       "summary  count               mean             stddev   min   max\n",
       "rides     8760  36.70616438356164  7.606905749665114    18    62\n",
       "hour      8760               11.5  6.922581688234331     0    23\n",
       "month     8760  6.526027397260274  3.448048134069238     1    12\n",
       "day       8760  15.72054794520548  8.796749115212322     1    31\n",
       "year      8760             2015.0                0.0  2015  2015"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features = [t[0] for t in weatherDF.dtypes if t[1] == 'int']\n",
    "weatherDF.select(numeric_features).describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- wind_direction: double (nullable = true)\n",
      " |-- rides: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- hour_sin: double (nullable = true)\n",
      " |-- hour_cos: double (nullable = true)\n",
      " |-- day_sin: double (nullable = true)\n",
      " |-- day_cos: double (nullable = true)\n",
      " |-- month_sin: double (nullable = true)\n",
      " |-- month_cos: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weatherDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "numeric_data = weatherDF.select('humidity','pressure','wind_speed', 'wind_direction', 'temperature').toPandas()\n",
    "axs = pd.plotting.scatter_matrix(numeric_data, figsize=(8, 8));\n",
    "n = 5\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using logesticregression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "numericCols = ['humidity', 'pressure', 'wind_speed', 'wind_direction', 'temperature']\n",
    "stages=[]\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "stages += [assembler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_stringIdx = StringIndexer(inputCol = 'rides', outputCol = 'label')\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(weatherDF)\n",
    "weatherDF = pipelineModel.transform(weatherDF)\n",
    "selectedCols = ['label', 'features']\n",
    "weatherDF = weatherDF.select(selectedCols)\n",
    "weatherDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 6111\n",
      "Test Dataset Count: 2649\n"
     ]
    }
   ],
   "source": [
    "train, test = weatherDF.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|[39.0,1013.0,6.0,...|[5.95412141251474...|[0.02599687243441...|       4.0|\n",
      "|  0.0|[39.0,1016.0,2.0,...|[6.05128577682234...|[0.03121381523653...|       4.0|\n",
      "|  0.0|[47.0,1018.0,4.0,...|[6.28271153017067...|[0.03740690783165...|       4.0|\n",
      "|  0.0|[48.0,1032.0,3.0,...|[6.63935356326205...|[0.04835149241832...|       1.0|\n",
      "|  0.0|[49.0,1018.0,4.0,...|[6.31458079496595...|[0.03867957542700...|       1.0|\n",
      "|  0.0|[51.0,1020.0,3.0,...|[6.38366345875886...|[0.04011805701094...|       1.0|\n",
      "|  0.0|[52.0,1013.0,4.0,...|[6.38983662623400...|[0.04170284321494...|       1.0|\n",
      "|  0.0|[52.0,1031.0,5.0,...|[6.71784348955568...|[0.05049672676987...|       1.0|\n",
      "|  0.0|[53.0,1017.0,2.0,...|[6.59559060332573...|[0.04945270171799...|       1.0|\n",
      "|  0.0|[53.0,1017.0,2.0,...|[6.58018063648273...|[0.04860972499488...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(test)\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 0.3027364105095715\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|[39.0,1013.0,6.0,...|[1.13934467634065...|[0.05696723381703...|       4.0|\n",
      "|  0.0|[39.0,1016.0,2.0,...|[0.88795305752630...|[0.04439765287631...|       8.0|\n",
      "|  0.0|[47.0,1018.0,4.0,...|[1.08713658586893...|[0.05435682929344...|       7.0|\n",
      "|  0.0|[48.0,1032.0,3.0,...|[0.95974154339131...|[0.04798707716956...|       8.0|\n",
      "|  0.0|[49.0,1018.0,4.0,...|[1.00457160125470...|[0.05022858006273...|       5.0|\n",
      "|  0.0|[51.0,1020.0,3.0,...|[2.19913302564692...|[0.10995665128234...|       4.0|\n",
      "|  0.0|[52.0,1013.0,4.0,...|[2.46112855782399...|[0.12305642789119...|       0.0|\n",
      "|  0.0|[52.0,1031.0,5.0,...|[2.06801941869780...|[0.10340097093489...|       5.0|\n",
      "|  0.0|[53.0,1017.0,2.0,...|[2.34450221207990...|[0.11722511060399...|       0.0|\n",
      "|  0.0|[53.0,1017.0,2.0,...|[2.25293923370725...|[0.11264696168536...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "rfModel = rf.fit(train)\n",
    "predictionsF = rfModel.transform(test)\n",
    "\n",
    "predictionsF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.14804339677838108\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictionsF, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.write.save('/home/negah.moharrami@analytique.desjardins.com/project/CEBD1261/data/prediction_taxi.csv',header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
